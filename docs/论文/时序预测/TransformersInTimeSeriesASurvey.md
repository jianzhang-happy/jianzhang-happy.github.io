# Transformers in Time Series: A Survey

[TOC]

| 阅读时间 | 2023-11-06 |
| -------- | ---------- |

## 论文主要内容

- 本文总结了Transformer架构在时间序列数据集上的三个主要应用：预测、异常检测、分类上的进展。

- 文章开头作者介绍了深度学习方法在这三个任务上的几个综述，强调本文侧重点在于介绍Transformer的应用。

- 文章从两个方面总结Transformer架构在时序数据上的应用：

  - 从模型层面：在模型层面上，作者回顾了基于Transformer架构在时序数据上的应用，目前学界所作的更改及其效果。其中模型更改可以分为从模块层面和从架构层面，绝大部分工作都是从模块层面对原始Transformer架构进行改进。模块层面的改进又包括几个方面，具体包括：

    - （1）位置编码的改进，原始Transformer架构的位置编码是基于绝对位置的编码，改进包括基于相对位置的编码，可学习编码以及时间戳编码；
    - （2）自注意力模块的改进：对自注意力模块的改进主要是为了降低其时间和空间计算复杂度。

    从架构层面的改进主要引入可以表征不同时间尺度的信息的能力。

  - 从应用层面：在应用层面上，作者回顾了Transformer在时序预测（包括时间序列预测、时空预测、事件预测）、异常检测和分类上的改进，具体包括：

    - 在预测任务上：
      - 时间序列预测：论述了包括模块层面和架构层面的改进；
      - 时空预测：主要介绍了Transformer和GNN结合的相关工作；
      - 事件预测：主要介绍了Transformer和时间点过程模型结合的相关工作；
    - 在异常检测任务上：主要介绍了将Transformer架构和对抗模型（VAE和GAN）结合的相关工作；
    - 在分类任务上：主要介绍了利用Transformer架构以及Transformer预训练进行分类的相关工作；

  - 同时，作者利用ETTm2数据集分析了不同Transformer架构在预测任务上的几个影响因素：

    - 序列长度的影响：在预测长度不变的情况下，当增大序列长度时，几个Transformer架构的表现反而更差了；
    - Transformer块层数的影响：在此数据集的情况下，最佳的层数是比较小的层数（3-6），更大的层数会引起结果变差；
    - 引入季节-趋势分解的影响：即使只引入简单的移动平均季节-趋势分解，预测表现也显著增强。

  - 最后，作者总结了几个Transformer架构在时序数据集上的未来研究方向：

    - 推导偏差（inductive biases）的问题；
    - 结合GNN的Transformer；
    - 预训练的Transformer；
    - 架构层面改进的Transformer；
    - Transformer的Neural architecture search（NAS）问题。

## 论文亮点

- 系统比较了不同Transformer改进模型在三类时序任务上的异同。

